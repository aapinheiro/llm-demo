{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks made by LLMs\n",
    "  - Sumarização\n",
    "  - Classificação \n",
    "  - Analise de Sentimetnos\n",
    "  - Perguntas e respostas \n",
    "  - Geração \n",
    "  - Tradução\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm-dev/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Walking amid Gion's Machiya wooden houses is a mesmerizing experience. The beautifully preserved structures exuded an old-world charm. The glow of lanterns lining the narrow streets add to the enchanting ambiance. Each stroll is a memorable journey through Japan's rich cultural history\"}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "text = \"\"\"Walking amid Gion's Machiya wooden houses is a mesmerizing experience. \n",
    "The beautifully preserved structures exuded an old-world charm that transports \n",
    "visitors back in time, making them feel like they had stepped into a living museum.\n",
    "The glow of lanterns lining the narrow streets add to the enchanting ambiance,\n",
    "making each stroll a memorable journey through Japan's rich cultural history.\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=60, length_penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Walking amid Gion's Machiya wooden houses is a mesmerizing experience. The beautifully preserved structures exuded an old-world charm. The glow of lanterns lining the narrow streets add to the enchanting ambiance. Each stroll is a memorable journey through Japan's rich cultural history\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Realizando uma tarefa de geração de texto \n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "text = \"You can make a good pasta just following these 4 steps\"\n",
    "\n",
    "output = generator(text, max_length=100, pad_token_id=generator.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é um Transformer?\n",
    "\n",
    "Uma arquitetura de Aprendizagem Profunda (Deep Learning) que processa, entende e gera textos. Essa arquitetura é usada em várias LLMs modernas. \n",
    "\n",
    "As arquiteturas mais comuns de LLMs são: \n",
    "- Encoder Only \n",
    "- Decoder Only\n",
    "- Encoder/Decoder Arch\n",
    "\n",
    "\n",
    "### Encoder Only \n",
    "Compreende o texto de input \n",
    "Não da como output uma saída sequencial \n",
    "Tasks mais comuns: \n",
    "    - Classificação de Textos \n",
    "    - Analise de sentimentos \n",
    "    - Extração de rótulos\n",
    "\n",
    "Tendem a ser BERT. \n",
    "\n",
    "### IDEIA DE PROJETO: \n",
    "Criar uma API que carrega um modelo do tipo Enconder only para classificar o sentimento de batchs de comentários de reviews de clientes. \n",
    "\n",
    "\n",
    "### Decoder Only \n",
    "Focado na geração de um output sequencial. \n",
    "- Task generation \n",
    "- Generative Question Answering \n",
    "\n",
    "Tendem a ser GPT\n",
    "\n",
    "\n",
    "### Enconder Decoder\n",
    "Entende o processo de entrada e saida. \n",
    "Geralmente utilizados para:\n",
    "- Tradução \n",
    "- Summarização de textos. \n",
    "\n",
    "Tendem a ser BART ou T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tunning \n",
    "\n",
    "#### LLM Lifecycle - \n",
    "Pre training -> Pre-trainide model \n",
    "- Broad data \n",
    "- Learn General Patterns \n",
    "\n",
    "Fine-Tuning -> Fine-tuned model \n",
    "- Domain Specific \n",
    "- Specialized tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m test_data \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mshard(num_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenzinzer the data \u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-dev/lib/python3.8/site-packages/transformers/utils/import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm-dev/lib/python3.8/site-packages/transformers/utils/import_utils.py:1630\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset(\"imdb\", split=\"train\")\n",
    "train_data = train_data.shard(num_shards=4, index=0)\n",
    "\n",
    "test_data = load_dataset(\"imdb\", split=\"train\")\n",
    "test_data = test_data.shard(num_shards=4, index=0)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenzinzer the data \n",
    "tokenized_trained_data = tokenizer(\n",
    "    train_data['text'], \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=64)\n",
    "\n",
    "tokenized_test_data = tokenizer(\n",
    "    test_data['text'], \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
